{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "557ec3ea-f827-4aea-a2bc-f02124dce825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from Bio import SeqIO\n",
    "import vcf\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer # For handling missing genotypes\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances # For distance calculation example\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import squareform # Needed if using scipy's dendrogram directly, but not for plotly's ff\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.express as px\n",
    "import time\n",
    "import traceback # For detailed error reporting\n",
    "import warnings # To suppress specific warnings if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38e01ae6-1613-4139-afd1-811cc5990113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_variants_in_region(vcf_dict, contig, start, end):\n",
    "\n",
    "    if not vcf_dict:\n",
    "        print(\"Warning: Input vcf_dict is empty.\")\n",
    "        return pd.DataFrame(columns=['CHROM', 'POS', 'REF', 'ALT',\n",
    "                                     'Absolute Count', 'Percentage (%)'])\n",
    "\n",
    "    \n",
    "    variant_occurrences = {}\n",
    "    total_vcfs = len(vcf_dict)\n",
    "\n",
    "    for vcf_name, record_list in vcf_dict.items():\n",
    "        processed_in_vcf = set() # Track variants processed for this specific VCF\n",
    "        for record in record_list:\n",
    "            # Check if the record is within the specified region\n",
    "            if record.CHROM == contig and start <= record.POS <= end:\n",
    "                # Ensure ALT is handled correctly (list of strings)\n",
    "                # Real pyvcf records have ALT as a list of Substitution objects\n",
    "                try:\n",
    "                    # Handle cases where ALT might be None or empty\n",
    "                    if record.ALT is None:\n",
    "                       alt_sequences = tuple()\n",
    "                    else:\n",
    "                       # Extract sequence string from each AltAllele/Substitution object\n",
    "                       alt_sequences = tuple(str(alt.sequence) for alt in record.ALT if alt is not None)\n",
    "\n",
    "                    # Create a unique identifier for the variant\n",
    "                    variant_key = (\n",
    "                        record.CHROM,\n",
    "                        record.POS,\n",
    "                        str(record.REF), # Ensure REF is a string\n",
    "                        alt_sequences    # Tuple of ALT strings\n",
    "                    )\n",
    "\n",
    "                    # Avoid double counting if a variant appears multiple times in the same sample VCF (shouldn't happen in standard VCFs)\n",
    "                    if variant_key not in processed_in_vcf:\n",
    "                        # Initialize the set for this variant if it's the first time seeing it\n",
    "                        if variant_key not in variant_occurrences:\n",
    "                            variant_occurrences[variant_key] = set()\n",
    "                        # Add the current VCF name to the set for this variant\n",
    "                        variant_occurrences[variant_key].add(vcf_name)\n",
    "                        processed_in_vcf.add(variant_key)\n",
    "\n",
    "                except AttributeError as e:\n",
    "                    print(f\"Warning: Skipping record due to unexpected structure \"\n",
    "                          f\"(maybe not a standard pyvcf Record?): {record}. Error: {e}\")\n",
    "                except Exception as e:\n",
    "                     print(f\"Warning: An unexpected error occurred processing record: {record}. Error: {e}\")\n",
    "\n",
    "    # Prepare data for the DataFrame\n",
    "    summary_data = []\n",
    "    if total_vcfs > 0:\n",
    "        for variant_key, vcf_set in variant_occurrences.items():\n",
    "            chrom, pos, ref, alts = variant_key\n",
    "            absolute_count = len(vcf_set)\n",
    "            percentage = (absolute_count / total_vcfs) * 100.0\n",
    "            summary_data.append({\n",
    "                'CHROM': chrom,\n",
    "                'POS': pos,\n",
    "                'REF': ref,\n",
    "                'ALT': ','.join(alts) if alts else '.', \n",
    "                'Absolute Count': absolute_count,\n",
    "                'Percentage (%)': round(percentage, 2) \n",
    "            })\n",
    "\n",
    "    # Create and sort the DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "    if not summary_df.empty:\n",
    "        summary_df = summary_df.sort_values(by=['CHROM', 'POS', 'REF', 'ALT']).reset_index(drop=True)\n",
    "        # Ensure correct data types\n",
    "        summary_df['POS'] = summary_df['POS'].astype(int)\n",
    "        summary_df['Absolute Count'] = summary_df['Absolute Count'].astype(int)\n",
    "\n",
    "\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3bf019b-42b5-4809-bdd4-a9528c889713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_fasta_string_with_biopython(fasta_string):\n",
    "    records = {}\n",
    "    fasta_handle = io.StringIO(fasta_string) # str je priamo v subore a nie priamo v premennej\n",
    "\n",
    "    for record in SeqIO.parse(fasta_handle, \"fasta\"): # Iteracia cez vsetky SeqIO zaznamy\n",
    "        records[record.id] = record\n",
    "    \n",
    "    fasta_handle.close() # Zatvorenie \"suboru\" s Fastou\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a16c394c-62bf-42ea-8307-73511b674379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vcf_string_with_pyvcf(vcf_string):\n",
    "    parsed_records = []\n",
    "    vcf_handle = io.StringIO(vcf_string)\n",
    "    vcf_reader = vcf.Reader(vcf_handle)\n",
    "    for record in vcf_reader:\n",
    "        parsed_records.append(record)\n",
    "    vcf_handle.close()\n",
    "    return parsed_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f308e51e-2f5a-4d87-894d-9aa20d2ffe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "contig_dropdown = widgets.Dropdown(\n",
    "    options=[], # Začína s prázdnymi možnosťami\n",
    "    description='Select Contig:',\n",
    "    disabled=True, # Začína ako neaktívny\n",
    "    style={'description_width': 'initial'}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8624e660-ec6a-4ebe-8fb4-3b27b7a12df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvorenie widgetu pre reference\n",
    "reference_uploader = widgets.FileUpload(\n",
    "    accept='fa',  \n",
    "    multiple=False,\n",
    "    description='Reference'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output() # Vytvorenie priestoru na výpis info o uploadoch (pre potreby vývoja)\n",
    "\n",
    "def handle_reference_upload(change):\n",
    "    global reference \n",
    "    reference = None\n",
    "    with output_area:\n",
    "        \n",
    "        clear_output(wait=True) # Vycistenie priestoru na vypis\n",
    "        contig_dropdown.options = []\n",
    "        contig_dropdown.value = None\n",
    "        contig_dropdown.disabled = True\n",
    "        uploaded_reference = change['new'] # ukladanie uploadovanych dat\n",
    "\n",
    "        if not uploaded_reference: # Kontrola na uploadnute data\n",
    "            print(\"No file selected or upload cancelled.\")\n",
    "            return\n",
    "        # Upload je zabalený v tuple \n",
    "        uploaded_reference = uploaded_reference[0] # Vytiahneme subor z tuple\n",
    "        \n",
    "        filename = uploaded_reference['name'] # Ulozenie mena suboru\n",
    "        content_memory_position = uploaded_reference['content'] # Odkaz na miesto v pamati, kde je obsah ulozeny\n",
    "        content = content_memory_position.tobytes() # Precitanie obsahu pamate v bytoch\n",
    "\n",
    "\n",
    "        try:\n",
    "            text_content = content.decode('utf-8') # Prelozenie bytoveho obsahu do textu. V text_content je teraz cely obsah nahranej referencky ako text.\n",
    "            reference = parse_fasta_string_with_biopython(text_content) # Ulozenie uploadovanej fasty do SeqIO formatu v globalnej premennej reference\n",
    "        except UnicodeDecodeError: # Ak nastala chyba pri preklade. To znamena, ze uploadnuty subor nebol korektny.\n",
    "           print(\"\\nCould not decode file as UTF-8 text.\")\n",
    "\n",
    "        if reference is not None: # Naplnenie dropdown menu\n",
    "            print(f\"Successfully loaded reference {filename}\")\n",
    "\n",
    "            \n",
    "            # Extrahovanie ID kontigov zo SeqRecord objektov\n",
    "            contig_names = list(reference.keys())\n",
    "            if contig_names:\n",
    "                contig_dropdown.options = contig_names # Nastavenie možností\n",
    "                contig_dropdown.value = contig_names[0] # Nastavenie predvolenej hodnoty\n",
    "                contig_dropdown.disabled = False # Aktivácia dropdownu\n",
    "            else:\n",
    "                print(\"V spracovaných záznamoch neboli nájdené žiadne ID kontigov.\")\n",
    "                # Dropdown ostáva neaktívny s prázdnymi možnosťami (už nastavené)\n",
    "\n",
    "        else:\n",
    "                print(\"Parsovanie zlyhalo (funkcia vrátila None). Dropdown nebude aktualizovaný.\")\n",
    "                # Dropdown ostáva neaktívny\n",
    "    return\n",
    "\n",
    "reference_uploader.observe(handle_reference_upload, names='value') # Nahranie akcie k upload widgetu.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b1c92dce-517c-448c-bf79-a1673f129bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vytvorenie widgetu pre vcfs\n",
    "vcfs_uploader = widgets.FileUpload(\n",
    "    accept='.vcf',  \n",
    "    multiple=True,\n",
    "    description='VCFs'\n",
    ")\n",
    "\n",
    "def handle_vcfs_upload(change):\n",
    "    global vcfs\n",
    "    vcfs = {}\n",
    "    with output_area:\n",
    "        \n",
    "        clear_output(wait=True) # Vycistenie priestoru na vypis\n",
    "\n",
    "        uploaded_vcfs = change['new'] # Sem sa nahraju uploadnute data\n",
    "\n",
    "        if not uploaded_vcfs: # Kontrola na uploadnute data\n",
    "            print(\"No file selected or upload cancelled.\")\n",
    "            return\n",
    "        else:\n",
    "            print(f\"Successfully uploaded {len(uploaded_vcfs)} VCFs.\")\n",
    "        for uploaded_vcf in uploaded_vcfs: # Upload je zabalený v tuple \n",
    "            \n",
    "            \n",
    "            filename = uploaded_vcf['name'] # Ulozenie mena suboru\n",
    "            content_memory_position = uploaded_vcf['content'] # Odkaz na miesto v pamati, kde je obsah ulozeny\n",
    "            content = content_memory_position.tobytes() # Precitanie obsahu pamate v bytoch\n",
    "            \n",
    "            try:\n",
    "                text_content = content.decode('utf-8') # Prelozenie bytoveho obsahu do textu. V text_content je teraz cely obsah nahranej referencky ako text.\n",
    "                vcfs[filename] = parse_vcf_string_with_pyvcf(text_content)\n",
    "            except UnicodeDecodeError: # Ak nastala chyba pri preklade. To znamena, ze uploadnuty subor nebol korektny.\n",
    "               print(\"\\nCould not decode file as UTF-8 text.\")\n",
    "    return\n",
    "\n",
    "vcfs_uploader.observe(handle_vcfs_upload, names='value') # Nahranie akcie k upload widgetu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "738d3360-fe14-4450-8e61-beb7597b4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_contig_change(change): # Zmeni hodnotu v start/stop poliach pri nastaveni kontigu\n",
    "    \"\"\"Updates start/end position widgets when dropdown selection changes.\"\"\"\n",
    "    selected_contig_id = change['new']\n",
    "\n",
    "    record = reference[selected_contig_id]\n",
    "    length = len(record.seq)\n",
    "\n",
    "    start_pos_widget.value = 1 # 1-based indexovanie, zrozumitelnejsie pre uzivatelov, co budu hlavne biologovia\n",
    "    end_pos_widget.value = length #1-based indexovanie, zrozumitelnejsie pre uzivatelov, co budu hlavne biologovia\n",
    "    \n",
    "    start_pos_widget.disabled = False\n",
    "    end_pos_widget.disabled = False\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "72f03228-b389-4c9e-b040-9a716f0b7459",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pos_widget = widgets.IntText(\n",
    "    value=0, # Startovacia hodnota, pri spusteni instancie je 0\n",
    "    description='Start Pos:',\n",
    "    disabled=True, # Kym nebude mat zmysel nieco zadavat\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "end_pos_widget = widgets.IntText(\n",
    "    value=0, # Startovacia hodnota, pri spusteni instancie je 0\n",
    "    description='End Pos (Length):',\n",
    "    disabled=True, # Kym nebude mat zmysel nieco zadavat\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "contig_dropdown.observe(on_contig_change, names='value') # Zmeny nastavaju pri zmene contigov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "249e64fb-317c-4159-919e-be659731c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_calculate_button(change):\n",
    "    with output_area:\n",
    "        clear_output(wait=True) # Vycistenie priestoru na vypis\n",
    "        df = summarize_variants_in_region(vcfs, contig_dropdown.value, start_pos_widget.value, end_pos_widget.value)\n",
    "        print(df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd2d3f64-48f5-4d37-9cf4-d7e00aa24413",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_button = widgets.Button(\n",
    "    description='Calculate',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    ")\n",
    "\n",
    "calculate_button.on_click(handle_calculate_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "711978ff-8166-4a9c-a246-21d7139f84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Analysis Function ---\n",
    "def run_pca_analysis_on_whole_genome(button_click_event):\n",
    "\n",
    "    output_area.clear_output() # Clear previous results\n",
    "\n",
    "    to_print = []\n",
    "\n",
    "    with output_area: # Redirect print statements and display calls here\n",
    "        start_time = time.time()\n",
    "        to_print.append(f\"Analysis started at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        to_print.append(\"=\"*50)\n",
    "        to_print.append(\"Starting analysis using pre-loaded VCF data...\")\n",
    "\n",
    "        # 0. Check if input dictionary exists\n",
    "        if 'vcfs' not in globals() or not isinstance(vcfs, dict) or not vcfs:\n",
    "            to_print.append(\"Error: The required dictionary 'vcfs' was not found or is empty.\")\n",
    "            to_print.append(\"Please ensure it is defined and populated in a previous cell before running this script.\")\n",
    "            return\n",
    "        else:\n",
    "            to_print.append(f\"   Found data for {len(vcfs)} samples in 'vcfs'.\")\n",
    "\n",
    "        try:\n",
    "            samples = list(vcfs.keys())\n",
    "            n_samples = len(samples)\n",
    "            if n_samples == 0:\n",
    "                to_print.append(\"Error: No samples found in 'vcfs'.\")\n",
    "                return\n",
    "\n",
    "            # ==================================\n",
    "            # === STEP 1: VARIANT FILTERING & MATRIX BUILDING ===\n",
    "            # ==================================\n",
    "            to_print.append(\"   Identifying unique variants passing filters in the specified region...\")\n",
    "            unique_variant_keys = set()\n",
    "            variants_passing_filters_initial = 0\n",
    "\n",
    "            for sample_id, records in vcfs.items():\n",
    "                for record in records: # record is a pyvcf.model._Record\n",
    "                    # MODIFIED FILTER: Include bi-allelic SNPs OR bi-allelic Indels\n",
    "                    if (record.is_snp or record.is_indel) and len(record.ALT) == 1:\n",
    "                        # Optional: check record.FILTER? ('PASS' or empty/None)\n",
    "                        # if record.FILTER is None or len(record.FILTER) == 0 or 'PASS' in record.FILTER:\n",
    "                            alt_str = str(record.ALT[0])\n",
    "                            variant_key = (record.CHROM, record.POS, record.REF, alt_str)\n",
    "                            unique_variant_keys.add(variant_key)\n",
    "                            variants_passing_filters_initial += 1\n",
    "\n",
    "            sorted_variant_keys = sorted(list(unique_variant_keys))\n",
    "            variant_to_index = {var_key: i for i, var_key in enumerate(sorted_variant_keys)}\n",
    "            n_variants = len(sorted_variant_keys)\n",
    "\n",
    "            # UPDATED PRINT STATEMENT\n",
    "            to_print.append(f\"   Found {n_variants} unique, filtered variants (bi-allelic SNPs or Indels) in the region.\")\n",
    "\n",
    "            if n_variants == 0:\n",
    "                to_print.append(\"Error: No suitable variants passed the filters in the specified region across all samples.\")\n",
    "                return\n",
    "\n",
    "            # --- Build Genotype Matrix ---\n",
    "            to_print.append(f\"   Building genotype matrix ({n_samples} samples x {n_variants} variants)...\")\n",
    "            genotype_matrix = np.full((n_samples, n_variants), -1, dtype=np.float32) # Fill with missing indicator -1\n",
    "            sample_to_index = {sample_id: i for i, sample_id in enumerate(samples)}\n",
    "\n",
    "            # Pre-build variant lookup for potentially faster access\n",
    "            variant_lookup = {}\n",
    "            for sample_id, records in vcfs.items():\n",
    "                for record in records:\n",
    "                     # MODIFIED FILTER (Consistent with above)\n",
    "                    if (record.is_snp or record.is_indel) and len(record.ALT) == 1:\n",
    "                        chrom_pos = (record.CHROM, record.POS)\n",
    "                        if chrom_pos not in variant_lookup: variant_lookup[chrom_pos] = {}\n",
    "                        variant_lookup[chrom_pos][sample_id] = record\n",
    "\n",
    "            # Populate the matrix\n",
    "            # NOTE: PyVCF's call.gt_type typically returns 0, 1, 2 for bi-allelic SNPs AND Indels,\n",
    "            # representing the count of the alternate allele. This encoding is used directly.\n",
    "            for var_idx, var_key in enumerate(sorted_variant_keys):\n",
    "                chrom, pos, ref, alt = var_key\n",
    "                chrom_pos = (chrom, pos)\n",
    "                if chrom_pos in variant_lookup:\n",
    "                    records_at_pos = variant_lookup[chrom_pos]\n",
    "                    for sample_id, record in records_at_pos.items():\n",
    "                        if record.REF == ref and str(record.ALT[0]) == alt: # Match exact variant\n",
    "                            sample_idx = sample_to_index[sample_id]\n",
    "                            if len(record.samples) > 0:\n",
    "                                call = record.samples[0] # Assuming single-sample VCFs in the dict values\n",
    "                                gt_type = call.gt_type # 0, 1, 2, or None (count of ALT allele)\n",
    "                                if gt_type is not None:\n",
    "                                    genotype_matrix[sample_idx, var_idx] = float(gt_type)\n",
    "                                # else: leave as -1 (missing)\n",
    "\n",
    "            # --- Matrix Post-Processing (Imputation, Zero-Variance Check) ---\n",
    "            to_print.append(\"\\nPost-processing genotype matrix...\")\n",
    "            # Handle Missing Data (Imputation)\n",
    "            genotype_matrix[genotype_matrix == -1] = np.nan # Convert placeholder to NaN\n",
    "            missing_values_count = np.isnan(genotype_matrix).sum()\n",
    "            if missing_values_count > 0:\n",
    "                to_print.append(f\"   Imputing {missing_values_count} missing genotypes using variant mean...\")\n",
    "                imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "                try:\n",
    "                    imputed_genotypes = imputer.fit_transform(genotype_matrix)\n",
    "                except ValueError as imp_err:\n",
    "                    # Check if imputation failed because all values in a column are NaN\n",
    "                    if \"Input contains NaN\" in str(imp_err) and np.isnan(genotype_matrix).all(axis=0).any():\n",
    "                         to_print.append(\"   Warning: At least one variant site has missing data for all samples.\")\n",
    "                         to_print.append(\"   Trying imputation column by column or removing such columns.\")\n",
    "                         # Option 1: Impute column by column (more robust but slower)\n",
    "                         # imputed_genotypes = genotype_matrix.copy()\n",
    "                         # for j in range(imputed_genotypes.shape[1]):\n",
    "                         #     col = imputed_genotypes[:, j].reshape(-1, 1)\n",
    "                         #     if np.isnan(col).any() and not np.isnan(col).all():\n",
    "                         #        imputer_col = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "                         #        imputed_genotypes[:, j] = imputer_col.fit_transform(col).flatten()\n",
    "                         #     elif np.isnan(col).all():\n",
    "                         #         print(f\"   Column {j} (variant {sorted_variant_keys[j]}) is all NaN - cannot impute mean, will be removed later if variance is zero.\")\n",
    "                         #         # Leave as NaN - zero variance check will handle it\n",
    "\n",
    "                         # Option 2: Remove columns that are all NaN before imputation (simpler)\n",
    "                         to_print.append(\"   Removing columns that consist entirely of missing data before imputation.\")\n",
    "                         all_nan_cols = np.where(np.isnan(genotype_matrix).all(axis=0))[0]\n",
    "                         if len(all_nan_cols) > 0:\n",
    "                              to_print.append(f\"   Indices of columns removed (all NaN): {all_nan_cols}\")\n",
    "                              # Keep track of which variants remain\n",
    "                              original_indices = np.arange(genotype_matrix.shape[1])\n",
    "                              valid_cols_indices = np.delete(original_indices, all_nan_cols)\n",
    "                              if len(valid_cols_indices) == 0:\n",
    "                                   to_print.append(\"Error: All variant columns consisted entirely of missing data. Cannot proceed.\")\n",
    "                                   return\n",
    "                              genotype_matrix_filtered_nan = genotype_matrix[:, valid_cols_indices]\n",
    "                              # Update sorted_variant_keys and variant_to_index if needed later (depends on downstream use)\n",
    "                              sorted_variant_keys = [sorted_variant_keys[i] for i in valid_cols_indices]\n",
    "                              variant_to_index = {var_key: i for i, var_key in enumerate(sorted_variant_keys)}\n",
    "                              n_variants = len(sorted_variant_keys) # Update variant count\n",
    "                              to_print.append(f\"   Proceeding with {n_variants} variants after removing all-NaN columns.\")\n",
    "                         else:\n",
    "                              genotype_matrix_filtered_nan = genotype_matrix # No columns were all NaN\n",
    "\n",
    "                         # Now impute the filtered matrix\n",
    "                         if np.isnan(genotype_matrix_filtered_nan).any(): # Check if any NaNs remain\n",
    "                             imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "                             imputed_genotypes = imputer.fit_transform(genotype_matrix_filtered_nan)\n",
    "                         elif genotype_matrix_filtered_nan.shape[1] > 0: # No NaNs left, but check if matrix is not empty\n",
    "                             imputed_genotypes = genotype_matrix_filtered_nan\n",
    "                         else: # Matrix became empty after removing all-NaN columns\n",
    "                             to_print.append(\"Error: No variants remaining after removing columns with only missing data.\")\n",
    "                             return\n",
    "\n",
    "                    else: # Different imputation error\n",
    "                        to_print.append(f\"Error during imputation: {imp_err}. Cannot proceed.\")\n",
    "                        traceback.print_exc()\n",
    "                        return\n",
    "            else:\n",
    "                to_print.append(\"   No missing genotypes found.\")\n",
    "                imputed_genotypes = genotype_matrix # No imputation needed\n",
    "\n",
    "            # Check for variants with zero variance AFTER imputation\n",
    "            to_print.append(\"   Checking for variants with zero variance...\")\n",
    "            if imputed_genotypes.shape[1] > 0:\n",
    "                variant_stds = np.nanstd(imputed_genotypes, axis=0) # Use nanstd for safety\n",
    "                variant_stds = np.nan_to_num(variant_stds, nan=0.0) # Replace potential NaNs (if column was all NaN and wasn't removed) with 0\n",
    "                non_zero_var_indices = np.where(variant_stds > 1e-6)[0]\n",
    "                zero_var_indices = np.where(variant_stds <= 1e-6)[0]\n",
    "                n_total_variants_after_imputation = imputed_genotypes.shape[1]\n",
    "                n_zero_variance = len(zero_var_indices)\n",
    "                n_non_zero_variance = len(non_zero_var_indices)\n",
    "                to_print.append(f\"   Found {n_non_zero_variance} variants with non-zero variance out of {n_total_variants_after_imputation}.\")\n",
    "            else:\n",
    "                to_print.append(\"Error: No variants available in the matrix after initial filtering and NaN handling.\")\n",
    "                return\n",
    "\n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "            #%%% BRANCH BASED ON VARIANCE PRESENCE %%%%%\n",
    "            #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "            # --- Handle Case: NO Variance Found ---\n",
    "            if n_non_zero_variance == 0:\n",
    "                to_print.append(\"\\n\" + \"=\"*50)\n",
    "                to_print.append(\"RESULT: All variants show NO VARIATION across samples (after imputation).\")\n",
    "                to_print.append(\"Samples are genetically identical based on the selected bi-allelic SNPs/Indels.\")\n",
    "                to_print.append(\"=\"*50)\n",
    "\n",
    "                # ==============================================\n",
    "                # === STEP 2: DISTANCE CALCULATION (No Variance Case) ===\n",
    "                # ==============================================\n",
    "                to_print.append(\"\\n--- Pairwise Euclidean Distances ---\")\n",
    "                # Matrix should have low/zero distances if no variance\n",
    "                if imputed_genotypes.shape[1] > 0:\n",
    "                    try:\n",
    "                        # Use the original imputed matrix (which has no variance here)\n",
    "                        euclidean_dist_matrix = pairwise_distances(imputed_genotypes, metric='euclidean')\n",
    "                        dist_df = pd.DataFrame(euclidean_dist_matrix, index=samples, columns=samples)\n",
    "                        to_print.append(\"   Distance Matrix (all samples identical = distances near zero):\")\n",
    "                        #display(dist_df.round(2))\n",
    "                    except Exception as dist_err:\n",
    "                        to_print.append(f\"   Could not calculate distances: {dist_err}\")\n",
    "                else:\n",
    "                    to_print.append(\"   Cannot calculate distances (no variants processed).\")\n",
    "\n",
    "                # ===========================\n",
    "                # === STEP 3: PCA (No Variance Case) ===\n",
    "                # ===========================\n",
    "                to_print.append(\"\\n--- Principal Component Analysis (PCA) ---\")\n",
    "                to_print.append(\"   Skipping PCA calculation as no variance was detected.\")\n",
    "                # --- Placeholder PCA Plot ---\n",
    "                to_print.append(\"\\n--- PCA Plot (Placeholder - No Variance) ---\")\n",
    "                dummy_pca_data = {'SampleID': samples, 'PC1': np.zeros(n_samples), 'PC2': np.zeros(n_samples)}\n",
    "                pca_df_dummy = pd.DataFrame(dummy_pca_data)\n",
    "                to_print.append(\"   PCA Coordinates (N/A - No Variance):\")\n",
    "                #display(pca_df_dummy)\n",
    "                try:\n",
    "                    fig = px.scatter(\n",
    "                        pca_df_dummy, x='PC1', y='PC2', hover_name='SampleID',\n",
    "                        title=f\"PCA Placeholder: No Variance Found (Whole genome)\",\n",
    "                        labels={'PC1': 'PC1 (N/A)', 'PC2': 'PC2 (N/A)'}, template='plotly_white'\n",
    "                    )\n",
    "                    fig.update_traces(marker=dict(size=8, opacity=0.8))\n",
    "                    fig.update_layout(xaxis_range=[-1,1], yaxis_range=[-1,1], hovermode='closest')\n",
    "                    fig.show(renderer=\"notebook\") # Ensure correct renderer for your environment\n",
    "                except Exception as plot_err:\n",
    "                    to_print.append(f\"   Could not generate placeholder PCA plot: {plot_err}\")\n",
    "\n",
    "\n",
    "                # ========================================================\n",
    "                # === STEP 4: Phylogenetic Clustering (No Variance Case) ===\n",
    "                # ========================================================\n",
    "                # **Attempt clustering even with no variance, as distances are defined (zero)**\n",
    "                to_print.append(\"\\n--- Phylogenetic Clustering (Hierarchical Dendrogram) ---\")\n",
    "                to_print.append(\"   Attempting to generate dendrogram based on (near) zero distances...\")\n",
    "                to_print.append(\"   NOTE: Based on Euclidean distance computed from imputed genotypes (0,1,2).\")\n",
    "\n",
    "                # Check if enough samples and variants exist\n",
    "                # Need at least 2 samples; variants don't need variance for distance=0\n",
    "                if imputed_genotypes.shape[0] >= 2 and imputed_genotypes.shape[1] >= 1:\n",
    "                    try:\n",
    "                        # Attempt to create dendrogram using the zero-variance imputed data\n",
    "                        fig_dendro = ff.create_dendrogram(\n",
    "                            imputed_genotypes, # Use original imputed data (no variance)\n",
    "                            orientation='left',\n",
    "                            labels=samples,\n",
    "                            linkagefun=lambda x: linkage(x, method='average', metric='euclidean') # Should result in zero-length branches\n",
    "                        )\n",
    "                        fig_dendro.update_layout(\n",
    "                            title_text=f\"Hierarchical Clustering Dendrogram<br>(UPGMA, Euclidean Distance - No Variance Detected)\",\n",
    "                            xaxis_title=\"Distance (Expected Near Zero)\",\n",
    "                            yaxis_title=\"Samples\",\n",
    "                            height=max(400, n_samples * 20),\n",
    "                            margin=dict(l=max(100, max(len(s) for s in samples)*7)) # Adjust left margin for labels\n",
    "                        )\n",
    "                        fig_dendro.show(renderer=\"notebook\") # Ensure correct renderer\n",
    "                        to_print.append(\"   Dendrogram generated (should show all samples clustered with near-zero distance).\")\n",
    "\n",
    "                    except Exception as dendro_err:\n",
    "                        to_print.append(f\"   Warning: Could not generate dendrogram, possibly due to lack of variance or plotting library limitations: {dendro_err}\")\n",
    "                        # traceback.print_exc() # Uncomment for detailed traceback if needed\n",
    "                else:\n",
    "                    # This case (no variants) should have been caught earlier, but added for robustness\n",
    "                    to_print.append(\"   Skipping dendrogram: Need at least 2 samples and 1 variant processed.\")\n",
    "\n",
    "            # --- Handle Case: SOME Variance Found ---\n",
    "            else: # n_non_zero_variance > 0\n",
    "                to_print.append(\"\\n\" + \"=\"*50)\n",
    "                to_print.append(\"RESULT: Variance detected. Proceeding with standard analysis.\")\n",
    "                to_print.append(\"=\"*50)\n",
    "\n",
    "                if n_zero_variance > 0:\n",
    "                    to_print.append(f\"\\nRemoving {n_zero_variance} zero-variance variants before proceeding.\")\n",
    "                    # Keep only columns (variants) with variance > 0\n",
    "                    imputed_genotypes_filtered = imputed_genotypes[:, non_zero_var_indices]\n",
    "                    # Update variant keys if needed for reporting (optional)\n",
    "                    # final_variant_keys = [sorted_variant_keys[i] for i in non_zero_var_indices]\n",
    "                else:\n",
    "                    imputed_genotypes_filtered = imputed_genotypes # Use the original imputed matrix\n",
    "                    # final_variant_keys = sorted_variant_keys # Use the keys after NaN filtering\n",
    "\n",
    "                if imputed_genotypes_filtered.shape[1] == 0: # Final check\n",
    "                     to_print.append(f\"Error: No variants remaining after filtering zero-variance columns.\")\n",
    "                     return\n",
    "\n",
    "                current_variants_used = imputed_genotypes_filtered.shape[1] # Number of variants used\n",
    "\n",
    "                # ==============================================\n",
    "                # === STEP 2: DISTANCE CALCULATION (Variance Case) ===\n",
    "                # ==============================================\n",
    "                to_print.append(\"\\n--- Calculating Pairwise Distances ---\")\n",
    "                to_print.append(f\"   (Calculating Euclidean distance on {current_variants_used} variants with non-zero variance)\")\n",
    "                try:\n",
    "                    euclidean_dist_matrix = pairwise_distances(imputed_genotypes_filtered, metric='euclidean')\n",
    "                    dist_df = pd.DataFrame(euclidean_dist_matrix, index=samples, columns=samples)\n",
    "                    to_print.append(\"   Pairwise Euclidean Distance Matrix (rounded):\")\n",
    "                    #display(dist_df.round(2))\n",
    "                except Exception as dist_err:\n",
    "                    to_print.append(f\"   Error calculating distances: {dist_err}\")\n",
    "\n",
    "                # --- PCA Data Preparation (Scaling) ---\n",
    "                to_print.append(\"\\nPreparing Data for PCA...\")\n",
    "                to_print.append(\"   Scaling data (centering and standardizing)...\")\n",
    "                scaler = StandardScaler()\n",
    "                try:\n",
    "                    # Ensure the filtered array is used here\n",
    "                    scaled_genotypes = scaler.fit_transform(imputed_genotypes_filtered)\n",
    "                    if np.isnan(scaled_genotypes).any():\n",
    "                        to_print.append(\"Error: NaN values detected after scaling. Check imputation/data.\")\n",
    "                        # This might indicate a problem with the scaler or input data\n",
    "                        return\n",
    "                except Exception as scale_err:\n",
    "                    to_print.append(f\"   Error during scaling: {scale_err}\")\n",
    "                    traceback.print_exc()\n",
    "                    return\n",
    "\n",
    "                # ===========================\n",
    "                # === STEP 3: PCA ===\n",
    "                # ===========================\n",
    "                to_print.append(\"\\nPerforming Principal Component Analysis (PCA)...\")\n",
    "                n_samples_pca, n_variants_pca = scaled_genotypes.shape\n",
    "                # Determine number of components (max 10, or limited by samples/variants)\n",
    "                n_components = min(10, n_samples_pca, n_variants_pca)\n",
    "\n",
    "                if n_variants_pca < 1: # Should have been caught, but double-check\n",
    "                    to_print.append(f\"Error: No variants available to perform PCA after all filtering.\")\n",
    "                    pca_success = False\n",
    "                elif n_components < 1: # Handle cases like 1 sample or 1 variant\n",
    "                    n_components = 1 # Calculate at least 1 PC if possible\n",
    "\n",
    "                pca_success = False # Flag to check if PCA ran\n",
    "                if n_variants_pca >= 1 and n_components >= 1:\n",
    "                    to_print.append(f\"   Running PCA with {n_components} components on {n_variants_pca} variants...\")\n",
    "                    try:\n",
    "                        pca = PCA(n_components=n_components)\n",
    "                        principal_components = pca.fit_transform(scaled_genotypes)\n",
    "                        pca_success = True # Mark PCA as successful\n",
    "\n",
    "                        # --- Prepare and Display PCA Results ---\n",
    "                        to_print.append(\"\\n--- PCA Results ---\")\n",
    "                        pc_cols = [f'PC{i+1}' for i in range(n_components)]\n",
    "                        pca_df = pd.DataFrame(data=principal_components, columns=pc_cols)\n",
    "                        pca_df['SampleID'] = samples\n",
    "                        pca_df = pca_df[['SampleID'] + pc_cols]\n",
    "                        explained_variance = pca.explained_variance_ratio_ * 100\n",
    "\n",
    "                        to_print.append(\"   PCA Coordinates (rounded):\")\n",
    "                        #display(pca_df.round(4))\n",
    "                        to_print.append(\"\\n   Explained Variance (%):\")\n",
    "                        variance_df = pd.DataFrame({\n",
    "                            'Principal Component': pc_cols,\n",
    "                            'Explained Variance (%)': explained_variance.round(2)\n",
    "                        })\n",
    "                        #display(variance_df)\n",
    "\n",
    "                        # --- Generate Interactive PCA Plot ---\n",
    "                        to_print.append(\"\\n--- Interactive PCA Plot ---\")\n",
    "                        if n_components >= 2:\n",
    "                            fig_pca = px.scatter(\n",
    "                                pca_df, x='PC1', y='PC2', hover_name='SampleID', hover_data=pc_cols,\n",
    "                                title=f\"PCA (Whole genome, {n_variants_pca} Variants used)\",\n",
    "                                labels={'PC1': f'PC1 ({explained_variance[0]:.2f}%)', 'PC2': f'PC2 ({explained_variance[1]:.2f}%)'},\n",
    "                                template='plotly_white'\n",
    "                            )\n",
    "                            fig_pca.update_traces(marker=dict(size=8, opacity=0.8))\n",
    "                            fig_pca.update_layout(hovermode='closest')\n",
    "                            fig_pca.show(renderer=\"notebook\") # Ensure correct renderer\n",
    "                        elif n_components == 1:\n",
    "                            to_print.append(\"   (Plotting PC1 only as only one component was calculated)\")\n",
    "                            pca_df['y_dummy'] = np.zeros(len(pca_df)) # Create dummy y-axis\n",
    "                            fig_pca = px.scatter(\n",
    "                                pca_df, x='PC1', y='y_dummy', hover_name='SampleID', hover_data=['PC1'],\n",
    "                                title=f\"PCA (Whole genome, {n_variants_pca} Variant) - PC1 Only\",\n",
    "                                labels={'PC1': f'PC1 ({explained_variance[0]:.2f}%)', 'y_dummy': ''},\n",
    "                                template='plotly_white'\n",
    "                            )\n",
    "                            fig_pca.update_traces(marker=dict(size=8, opacity=0.8))\n",
    "                            fig_pca.update_layout(hovermode='closest', yaxis_visible=False, yaxis_showticklabels=False)\n",
    "                            fig_pca.show(renderer=\"notebook\") # Ensure correct renderer\n",
    "\n",
    "                    except Exception as pca_err:\n",
    "                        to_print.append(f\"   Error during PCA calculation or plotting: {pca_err}\")\n",
    "                        traceback.print_exc() # Uncomment for detailed traceback\n",
    "\n",
    "                # ========================================================\n",
    "                # === STEP 4: Phylogenetic Clustering (Variance Case) ===\n",
    "                # ========================================================\n",
    "                to_print.append(\"\\n--- Phylogenetic Clustering (Hierarchical Dendrogram) ---\")\n",
    "                to_print.append(\"   NOTE: Using UPGMA ('average' linkage) based on Euclidean distance computed from imputed genotypes (0,1,2).\")\n",
    "\n",
    "                # Check if enough samples and variants exist for clustering\n",
    "                if imputed_genotypes_filtered.shape[0] >= 2 and imputed_genotypes_filtered.shape[1] >= 1:\n",
    "                    to_print.append(f\"   Generating dendrogram for {n_samples} samples using {current_variants_used} variants...\")\n",
    "                    try:\n",
    "                        # Use imputed data *before* scaling for dendrogram based on genotype similarity\n",
    "                        fig_dendro = ff.create_dendrogram(\n",
    "                            imputed_genotypes_filtered, # (samples x variants)\n",
    "                            orientation='left',\n",
    "                            labels=samples,\n",
    "                            linkagefun=lambda x: linkage(x, method='average', metric='euclidean') # UPGMA\n",
    "                        )\n",
    "                        fig_dendro.update_layout(\n",
    "                            title_text=f\"Hierarchical Clustering Dendrogram<br>(UPGMA, Euclidean Distance, {current_variants_used} Variants)\",\n",
    "                            xaxis_title=\"Distance\", yaxis_title=\"Samples\",\n",
    "                            height=max(400, n_samples * 20), # Adjust height based on samples\n",
    "                            margin=dict(l=max(100, max(len(s) for s in samples)*7)) # Adjust left margin\n",
    "                        )\n",
    "                        fig_dendro.show(renderer=\"notebook\") # Ensure correct renderer\n",
    "\n",
    "                    except Exception as dendro_err:\n",
    "                        to_print.append(f\"   Error generating dendrogram: {dendro_err}\")\n",
    "                        # traceback.print_exc() # Uncomment for detailed traceback\n",
    "                else:\n",
    "                     to_print.append(\"   Skipping dendrogram: Need at least 2 samples and 1 variant with variance.\")\n",
    "\n",
    "            # --- End of the 'else' block for handling non-zero variance ---\n",
    "\n",
    "            # Final completion message outside the if/else\n",
    "            end_time = time.time()\n",
    "            to_print.append(\"\\n\" + \"=\"*50)\n",
    "            to_print.append(f\"Analysis run finished in {end_time - start_time:.2f} seconds.\")\n",
    "            to_print.append(f\"Finished at: {time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            to_print.append(\"=\"*50)\n",
    "\n",
    "        # --- General Error Handling ---\n",
    "        except ImportError as ie:\n",
    "            to_print.append(f\"Import Error: {ie}. Please ensure required libraries are installed:\")\n",
    "            to_print.append(\"   pip install jupyter voila ipywidgets numpy pandas scikit-learn plotly PyVCF scipy ipython\") # Added ipython for display\n",
    "        except KeyError as ke:\n",
    "             to_print.append(f\"Key Error: {ke}. Problem accessing data, check dictionary structure or sample names.\")\n",
    "             traceback.print_exc()\n",
    "        except ValueError as ve:\n",
    "            to_print.append(f\"Value Error during processing: {ve}\")\n",
    "            to_print.append(\"   Check VCF format, region, numeric conversion, or data consistency.\")\n",
    "            traceback.print_exc()\n",
    "        except MemoryError as me:\n",
    "            to_print.append(f\"Memory Error: {me}. The dataset for the selected region might be too large.\")\n",
    "            to_print.append(\"   Try a smaller region or run on a machine with more RAM.\")\n",
    "        except Exception as e:\n",
    "            to_print.append(f\"An unexpected error occurred: {e}\")\n",
    "            to_print.append(\"\\n--- Full Traceback ---\")\n",
    "            traceback.print_exc() # Print detailed traceback\n",
    "        print(\"\\n\".join(to_print))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "53403fb5-54d3-4942-9b24-6fe96749b913",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_button = widgets.Button(\n",
    "    description='Create clusters',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    ")\n",
    "\n",
    "cluster_button.on_click(run_pca_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb31cacf-4aa5-4e86-a482-3ce971dc6e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_all_button = widgets.Button(\n",
    "    description='Create clusters on whole genome',\n",
    "    disabled=False,\n",
    "    button_style='success',\n",
    ")\n",
    "\n",
    "cluster_all_button.on_click(run_pca_analysis_on_whole_genome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "814da03b-e02e-469f-944d-895d19439695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218d9dfcce0e499c943fbeedb73b2195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(VBox(children=(Label(value='Input Files:'), FileUpload(value=(), accept='fa', description='Refe…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "file_inputs = widgets.VBox([\n",
    "    widgets.Label(\"Input Files:\"),\n",
    "    reference_uploader,\n",
    "    vcfs_uploader\n",
    "], layout=widgets.Layout(margin='0 0 10px 0'))\n",
    "\n",
    "\n",
    "region_selection = widgets.VBox([\n",
    "    widgets.Label(\"Genomic Region:\"),\n",
    "    contig_dropdown,\n",
    "    widgets.HBox([start_pos_widget, end_pos_widget])\n",
    "], layout=widgets.Layout(margin='0 0 10px 0'))\n",
    "\n",
    "action_buttons = widgets.HBox([\n",
    "    calculate_button,\n",
    "    cluster_button,\n",
    "    cluster_all_button\n",
    "], layout=widgets.Layout(margin='0 0 10px 0')) # Add some space below\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    file_inputs,\n",
    "    region_selection,\n",
    "    action_buttons,\n",
    "    widgets.HTML(\"<hr>\"), # Add a visual separator\n",
    "    output_area\n",
    "])\n",
    "\n",
    "display(ui)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
